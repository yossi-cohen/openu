{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The field of **Dimensionality Reduction** is all about *discovering non-linear relationships in data\n",
    "<br> that are not obvious in the original feature space.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Why do we need Dimentionality Reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Visualization**: if we reduce the number of dimentions in data into 2 or 3 space, we can visualize it. \n",
    "  <br>This helps us to analyze the data\n",
    "<br>\n",
    "<br>\n",
    "- **Data Compression**\n",
    "    - easier storage and processing of data\n",
    "<br>\n",
    "<br>\n",
    "- **High dimention data has problems**:\n",
    "    - high time/space complexity\n",
    "    - prone to overfitting\n",
    "    - not all features are relevant, thus if we can reduce the dimention we can reduce the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Reducing from D=2 into d=1\n",
    "- rather then representing each point with **two** coordinates\n",
    "    <br> we can represent each point by **one** coordinate (corresponding to its projection on the red line)\n",
    "- by doing this we incure a bit of error as these points do not exactly lie on the red line.\n",
    "\n",
    "### Reducing from D=3 into d=2\n",
    "- similarly, in the drawing on the right, we can see that the data rufly lies on a surface. Tuhs can be described using only 2 dimentions.\n",
    "\n",
    "<img src='images/pca1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "The variance is the measure of **how spread out** some data is.\n",
    "<br>(the average distance of the data point to the mean)\n",
    "- $variance = (std)^2$\n",
    "\n",
    "<img src='images/variance.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors and Eigenvalues\n",
    "(eigen is the word 'characteristic' in German)\n",
    "\n",
    "A (non-zero) vector $v$ of dimension $N$ is an **eigenvector** of a square $N × N$ matrix $A$ if it satisfies the linear equation: $Av =\\lambda v$\n",
    "<br>where $λ$ is a scalar, termed the **eigenvalue** corresponding to $v$.\n",
    "\n",
    "That is, the *eigenvectors* are the vectors that the linear transformation $A$ merely **elongates or shrinks**, \n",
    "<br>and the amount that they elongate/shrink by is the *eigenvalue*.\n",
    "\n",
    "<img src='images/eigen-decomposition.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# PCA\n",
    "\n",
    "PCA is used as a method for Dimentionality Reduction\n",
    "\n",
    "- PCA transorms a set of variables into a new set of variables which are a linear combination of the original variables.\n",
    "- These new variables are known as **Principle Components**.\n",
    "- PCA is an ortogonal linear transformation that transform data into a new coordinate system such that \n",
    "    - the gratest variance by some projection of the data lies on the **first** principle component\n",
    "    - the second gratest variance lies on the **second** principle component\n",
    "    - etc.\n",
    "\n",
    "<img src='images/scree.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data standardization\n",
    "Our first step before we do PCA is to standardize data.\n",
    "<br>We need to standardize data because (TODO: why?)\n",
    "<br>Standardizing means **puting the data on the same scale**\n",
    "<br> This means our data should have mean = 0 and variance = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximize variance\n",
    "PCA is a variance maximizing exersize.\n",
    "<br>It projects the original data on a direction that maximizes varience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a rank of a matrix\n",
    "<img src='images/rank.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is what will be presented\n",
    "- First we'll make some data to apply PCA to\n",
    "- How to use **PCA()** function from sklearn to do PCA.\n",
    "- How to determine how much variation each principle component accounts for\n",
    "- How to draw PCA graph using matplotlib\n",
    "- Ho to examine the loading scores to determine what variables have the largest effect on the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing # for scaling the data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an array of 100 gene names (row names)\n",
    "genes = ['gene' + str(i) for i in range(1,101)]\n",
    "\n",
    "# we now create arrays of sample names (column names)\n",
    "# we have 5 'wild types' or 'wt' samples and 5 'knock out' or 'ko' samples\n",
    "wt = ['wt' + str(i) for i in range(1,6)]\n",
    "ko = ['ko' + str(i) for i in range(1,6)]\n",
    "\n",
    "# create a dataframe to store the made-up data \n",
    "# (the starts unpack the wt and ko arrays)\n",
    "data = pd.DataFrame(columns=[*wt, *ko], index=genes)\n",
    "\n",
    "# for each gene in the index, we create 5 values for the 'wt' samples and 5 values for the 'ko' samples\n",
    "# the made up data comes from two poisson distributions (one for the 'wt' samples and the other for 'ko')\n",
    "# for each gene we select a new mean for the poisson distribution (the means can vary between 10-1000)\n",
    "for gene in data.index:\n",
    "    data.loc[gene,'wt1':'wt5'] = np.random.poisson(lam=rd.randrange(10,1000), size=5)\n",
    "    data.loc[gene,'ko1':'ko5'] = np.random.poisson(lam=rd.randrange(10,1000), size=5)\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scale the data\n",
    "before we do PCA we have to scale and center our data\n",
    "<br>after scaling: the average value for each gene will be 0 and the standard deviation will be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (the sacle function expect samples as rows instead of columns)\n",
    "scaled_data = preprocessing.scale(data.T)\n",
    "\n",
    "# you can also use: scaled_data = preprocessing.StandardScaler().fit_transform(data.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "# this is where all the math goes, \n",
    "# (i.e: calculate loading scores and the variation each principle component accounts for)\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# this is where we generate coordinates for the PCA graph based on the loading scores and the scaled data.\n",
    "pca_data = pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw a scree plot\n",
    "we'll start with a scree plot to see how many principle components should we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of variation each principle component accounts for\n",
    "per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
    "\n",
    "labels = ['PC' + str(i) for i in range(1, len(per_var) + 1)] # 'PC1', 'PC2', etc.\n",
    "\n",
    "plt.bar(x=range(1, len(per_var) + 1), height=per_var, tick_label=labels)\n",
    "\n",
    "plt.ylabel('Percentage of explained variance')\n",
    "plt.xlabel('Principle component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "# we can see the almost all of the variation goes into the first principle component, so\n",
    "# a 2-D graph should do a good job in representing the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the PCA graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll put the new coordinates created by pca.transform(scaled_data) into a matrix\n",
    "# where the rows have sample labels and the columns have PC labels\n",
    "pca_df = pd.DataFrame(pca_data, index=[*wt, *ko], columns=labels)\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_df.PC1, pca_df.PC2)\n",
    "plt.title('PCA plot')\n",
    "plt.xlabel('PC1 - {}%'.format(per_var[0]))\n",
    "plt.ylabel('PC2 - {}%'.format(per_var[1]))\n",
    "\n",
    "# add sample names to the graph\n",
    "for sample in pca_df.index:\n",
    "    plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ko' samples are clustered on the left, suggesting they are correlated with each other.\n",
    "\n",
    "The 'wt' samples are clustered on the right, also correlated with each other.\n",
    "\n",
    "Lastly, lets look at the loading scores for PC1 to determine which genes had the largest influence\n",
    "<br> on separating the two clusters along the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_scores = pd.Series(pca.components_[0], index=genes)\n",
    "sorted_loading_scores = loading_scores.abs().sort_values(ascending=False)\n",
    "top_10_genes = sorted_loading_scores[0:10].index.values\n",
    "print(sorted_loading_scores[top_10_genes])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
